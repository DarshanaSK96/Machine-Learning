{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM  Hard Allocation\n",
    " <h4>Implement  GMM  hard allocation procedure to  cluster  data<h4/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries required for my code to run\n",
    "\n",
    "#Uncomment this if you want to display your [graphs] within the notebook in a proper format.\n",
    "%matplotlib inline\n",
    "#Uncomment this if you want to display your graphs in backend\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Synthetic Data Generation first method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthetic Data Generation first method\n",
    "\n",
    "K=4 # Step1:Choose the Number of clusters\n",
    "#%matplotlib\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "Data, y_true = make_blobs(n_samples=400, centers=K,\n",
    "                       cluster_std=0.70, random_state=0)\n",
    "\n",
    "df = pd.DataFrame(data=Data, columns=[\"X\", \"Y\"])\n",
    "#Plot 2D\n",
    "\n",
    "plt.scatter(Data[:, 0], Data[:, 1], s=5)\n",
    "\n",
    "    \n",
    "centers = df.mean(0)\n",
    "Initial_Centroid=centers.to_numpy()\n",
    "print(Initial_Centroid)\n",
    "\n",
    "plt.scatter(centers['X'],centers['Y'],c='red', s=50)\n",
    "plt.title(\"SampleDataGeneration-2D\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.grid()\n",
    "#plt.scatter(centers[:, 0], centers[:, 1],c='red', s=50)\n",
    "#plt.title(\"SampleDataGeneration-2D\")\n",
    "#plt.xlabel(\"X-axis\")\n",
    "#plt.ylabel(\"Y-axis\")\n",
    "#plt.grid()\n",
    "\n",
    "#3D Plot\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(Data[:, 0],Data[:, 1],\n",
    "           linewidths=1, alpha=0.5,\n",
    "           #edgecolor='k',\n",
    "           s =10,\n",
    "           )\n",
    "ax.set_title(\"SampleDataGeneration-3D\")\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def plot_hist(data):\n",
    "    for x in data:\n",
    "        plt.hist(x, bins = 80, alpha = 0.7)\n",
    "\n",
    "plot_hist([Data[:, 0], Data[:, 1]])\n",
    "plt.title(\"Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(Dataset,Centroids):\n",
    "    \n",
    "    from sklearn.metrics import pairwise_distances_argmin\n",
    "    labels=pairwise_distances_argmin(Dataset,Centroids)\n",
    "    print(labels)\n",
    "\n",
    "    return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Kmeans(X, clusters):\n",
    "    # Step2. Randomly choose initial Centroids\n",
    "    \n",
    "    Initial_Centroids=InitialCents\n",
    "    print(\"In\",Initial_Centroids)\n",
    "    clusters=len(Initial_Centroids)\n",
    "    #print(\"Initial Centroids are:\\n\",Initial_Centroids)\n",
    "    #I am capturing dataset in excel.Making sure if that already exists.If exists,overwrite\n",
    "    #If not,create new\n",
    "    import glob\n",
    "    import sys\n",
    "    #fileList = glob.glob('C:\\dataset*.xlsx')\n",
    "    j=0\n",
    "    while True:       \n",
    "        print(\"Iteration\",j)\n",
    "        # Step2a: Assign labels to datapoints or assign datapoints\n",
    "        #to cluster based on minimum distance to the closest-centeroid\n",
    "        \n",
    "        labels = EuclideanDistance(X,Initial_Centroids)\n",
    "        #print(labels)\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(data=X, columns=[\"X\", \"Y\"])\n",
    "        \n",
    "        df['Cluster']=labels\n",
    "        #Capturing my data in excel.Whoevere uses this give the path name as per your sytem settings\n",
    "        #Validation part.uncomment below line.\n",
    "        #df.to_excel (r'C:\\dataset'+str(j)+'.xlsx', index = True, header=True)\n",
    "       \n",
    "        # Step3: Find new centeroids by calculating\n",
    "        # Means of Datapoints belonging to the same cluster \n",
    "        \n",
    "        New_Centroids = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(clusters)])\n",
    "        print(\"New_Centroids\\n\",New_Centroids)\n",
    "        #df1 = pd.DataFrame(data=New_Centroids, columns=[\"X\", \"Y\"])\n",
    "        \n",
    "         #Capturing my centroids in excel.Whoevere uses this give the path name as per your sytem settings\n",
    "        #Validation part.uncomment below line.\n",
    "        #df1.to_excel (r'C:\\Centroids'+str(j)+'.xlsx', index = True, header=True)\n",
    "        \n",
    "        # Step4: Check if previous-centroids and new-centroids are equal\n",
    "        #print(centers)\n",
    "        #print(new_centers)\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        if np.all((Initial_Centroids == New_Centroids)):\n",
    "            break\n",
    "        j=j+1    \n",
    "        Initial_Centroids = New_Centroids\n",
    "       \n",
    "    \n",
    "    return New_Centroids, labels\n",
    "\n",
    "\n",
    "#print(centers)\n",
    "#print(labels)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Split Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "while True:\n",
    "    \n",
    "    print(\"IterationBinarySplit\",count)\n",
    "   \n",
    "    c=[]\n",
    "    #size=Initial_Centroid.ndim\n",
    "    print(Initial_Centroid)\n",
    "    for i in range(1):\n",
    "        InitialCentroid1=Initial_Centroid+0.002\n",
    "        cent=c.append(InitialCentroid1)\n",
    "        InitialCentroid2=Initial_Centroid-0.002\n",
    "        cent=c.append(InitialCentroid2)\n",
    "    InitialCents=np.vstack(c)  \n",
    "    print(\"My BinarySplit Centroid of size\",len(InitialCents))\n",
    "    print(InitialCents)\n",
    "    sie=len(InitialCents)\n",
    "    if(sie==2*K):\n",
    "        print(\"\\n\\nSince the size of centroids after binary split is twice the number of clusters\")\n",
    "        print(\"\\nHence,exit the code as i met with my convergence criteria!!\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    print(sie)\n",
    "    condtn=2*K\n",
    "    #print(condtn)\n",
    "    print(\"\\nEntering into Kmeans algorithm\")\n",
    "    Codebook,labels = Kmeans(Data,K)\n",
    "        #print(Codebook)\n",
    "    Initial_Centroid=Codebook\n",
    "        #print(Initial_Centroid)\n",
    "    centcount=len(Codebook)\n",
    "    print(centcount)\n",
    "   \n",
    "    \n",
    "    if(sie!=(2*K)):\n",
    "        \n",
    "    \n",
    "        Codebook=Initial_Centroid\n",
    "        count=count+1 \n",
    "        print(count)\n",
    "        continue\n",
    "       \n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(\"\\n\\nMy final Converged centroids are:\",Codebook)     \n",
    "       \n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.scatter(Data[:, 0], Data[:, 1],s=10,c=labels,cmap='viridis');\n",
    "plt.scatter(Codebook[:, 0], Codebook[:, 1], c='Red', s=150, alpha=0.7);\n",
    "plt.title(\"CodebookResult-2D\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "X=[]\n",
    "\n",
    "for i in range(K):\n",
    "    Data1= np.array(Data[labels == i])\n",
    "    X.append(Data1)\n",
    "    print(\"Data\"+str(j),Data1)\n",
    "    j=j+1\n",
    "X=np.vstack(X)    \n",
    "#print(\"KMeans:\",X)\n",
    "#print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "<h2>-----------------------------------------------------------------------------------------</h2>\n",
    " <h2>GMM Begins from here</h2>\n",
    "-------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here i am calculating the parameters of Kmeans clusters required for GMM\n",
    "X=Data\n",
    "data = {\n",
    "...     'X': ['Xavier', 'Ann', 'Jana', 'Yi', 'Robin', 'Amal', 'Nori'],\n",
    "...     'Y': ['Mexico City', 'Toronto', 'Prague', 'Shanghai',\n",
    "\n",
    "}\n",
    "def calculate_mean_covariance_weight(X, prediction):\n",
    "    C = K\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    gaussians= np.unique(prediction)\n",
    "    \n",
    "    initial_means = np.zeros((C, d))\n",
    "    initial_cov = np.zeros((C, d, d))\n",
    "    initial_pi = np.zeros(C)\n",
    "        \n",
    "    counter=0\n",
    "    for label in sorted(gaussians):\n",
    "        ids = np.where(prediction == label) # returns indices\n",
    "        print(\"Ids of class/label\"+str(counter),ids)\n",
    "        initial_pi[counter] = len(ids[0]) / X.shape[0] \n",
    "        print(initial_pi[counter])\n",
    "        initial_means[counter,:] = np.mean(X[ids], axis = 0)\n",
    "        de_meaned = X[ids] - initial_means[counter,:]\n",
    "        Nk = X[ids].shape[0]\n",
    "        print(Nk,X.shape[0])\n",
    "        initial_cov[counter,:, :] = np.dot(initial_pi[counter] * de_meaned.T, de_meaned) / Nk\n",
    "        counter+=1\n",
    "       \n",
    "    assert np.sum(initial_pi) == 1\n",
    "    return (initial_means, initial_cov, initial_pi)\n",
    "    \n",
    "\n",
    "prediction = labels\n",
    "\n",
    "m, c, pi = calculate_mean_covariance_weight(X, prediction)\n",
    "print(\"Initial mean:\",m)\n",
    "print(\"Initial Covariance matrix\",c)\n",
    "print(\"Initial pi/weight counter:\",pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "X=Data\n",
    "\n",
    "class GaussianMixture():\n",
    "    #Here you will create a refernce to all the parameters which gets substituted against declared variables\n",
    "    def __init__(self, gaussians: int, n_iters: int, tol: float, seed: int):\n",
    "        self.gaussians = gaussians\n",
    "        self.n_iters = n_iters\n",
    "        self.tol = tol\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        # data's dimensionality and probability vector initialization\n",
    "        self.n_row, self.n_col = X.shape     \n",
    "        self.probability = np.zeros((self.n_row, self.gaussians))\n",
    "        \n",
    "        #print(self.probability)\n",
    "        \n",
    "        ##Below multicommented block can be used if you want to apply GMM on a dataset without Kmeans result\n",
    "        \n",
    "        \"\"\"\n",
    "        # initialize parameters\n",
    "        np.random.seed(self.seed)\n",
    "        chosen = np.random.choice(self.n_row, self.gaussians, replace = False)\n",
    "        #print(\"Chosen:\",chosen)\n",
    "        self.means = X[chosen]\n",
    "        #print(\"Initial Means:\",self.means)\n",
    "        self.weights = np.full(self.gaussians, 1 / self.gaussians)\n",
    "        #print(\"Initial weights:\",self.weights)\n",
    "        \n",
    "        # for np.cov, rowvar = False, \n",
    "        # indicates that the rows represents obervation\n",
    "        shape = self.gaussians, self.n_col, self.n_col\n",
    "        self.covs = np.full(shape, np.cov(X, rowvar = False))\n",
    "       # print(\"Initial Covariance:\",self.covs)\n",
    "        \"\"\"\n",
    "        self.means=m\n",
    "        self.weights=pi\n",
    "        self.covs=c\n",
    "        \n",
    "\n",
    "        log_likelihood = 0 #Initializing for iteration\n",
    "        self.converged = False\n",
    "        self.log_likelihood_trace = []      \n",
    "        print(\"...Entering GMM Clustering...\\n\")\n",
    "        for i in range(self.n_iters):\n",
    "            \n",
    "            log_likelihood_new = self.Estep(X)\n",
    "            self.Mstep(X)\n",
    "            \n",
    "\n",
    "            if  (abs(log_likelihood_new - log_likelihood) <= self.tol):\n",
    "                \n",
    "                self.converged = True\n",
    "                break\n",
    "  \n",
    "            log_likelihood = log_likelihood_new\n",
    "            self.log_likelihood_trace.append(log_likelihood)\n",
    "            print(\"Iteration: \",i,\"  log_likelihood: \", log_likelihood)\n",
    "        \n",
    "        plt.plot(self.log_likelihood_trace)\n",
    "        plt.title(\"Loglikelihood Convergence Graph\")\n",
    "        \n",
    "        \n",
    "        #print(\"log_likelihood_trace:\",self.log_likelihood_trace)\n",
    "        last=self.log_likelihood_trace[-1]\n",
    "        #print(last)\n",
    "\n",
    "        return self.means,self.weights,self.covs,self.probability\n",
    "\n",
    "    def Estep(self, X):\n",
    "        \"\"\"\n",
    "        E-step: compute probability,\n",
    "        update probability matrix so that probability[i, j] is the probability of cluster k \n",
    "        for data point i,\n",
    "        to compute likelihood of data point i belonging to given cluster k, \n",
    "        use multivariate_normal.pdf\n",
    "        \"\"\"\n",
    "        self._compute_log_likelihood(X)\n",
    "        \n",
    "        self.log_likelihood1 = np.sum(np.log(np.sum(self.probability, axis = 1)))\n",
    "        \n",
    "         #Normalization       \n",
    "        self.probability = self.probability / self.probability.sum(axis = 1, keepdims = 1)\n",
    "        #print(\"Normalised probability\",self.probability)\n",
    "        return self.log_likelihood1\n",
    "\n",
    "    def _compute_log_likelihood(self, X):\n",
    "        for k in range(self.gaussians):\n",
    "            \n",
    "                prior = self.weights[k]\n",
    "                #print(\"prior_weight\",prior)\n",
    "                likelihood = multivariate_normal(self.means[k], self.covs[k]).pdf(X)\n",
    "                #print(\"Likelihood/probability\"+str(k),likelihood)\n",
    "                self.probability[:, k] = prior * likelihood\n",
    "                #print(\" Size of Initial Probability of all the datapoints in cluster\"+str(k),self.probability.shape)          \n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def compute_log_likelihood(self, X):\n",
    "        self.probs = np.zeros((X.shape[0] , self.gaussians))\n",
    "        \n",
    "        for k in range(self.gaussians):\n",
    "\n",
    "            prior = self.weights[k]            \n",
    "            #print(\"prior_weight\",prior)\n",
    "            self.likeli = multivariate_normal(self.means[k], self.covs[k]).pdf(X)\n",
    "            #print(\"Likelihood/probability\"+str(k),likelihood)\n",
    "\n",
    "            self.probs[:,k]= prior * self.likeli\n",
    "            #print(\" Size of Initial Probability of all the datapoints in cluster\"+str(k),self.probability.shape)       \n",
    "\n",
    "        self.probs = self.probs / (np.sum(self.probs, axis=1)[:, np.newaxis])\n",
    "        \n",
    "        return self.probs\n",
    "\n",
    "\n",
    "\n",
    "    def compute_log_likelihood_newmean(self, X, nmean, nvar, nweights):\n",
    "        self.probs1 = np.zeros((X.shape[0], self.gaussians))\n",
    "        \n",
    "        for k in range(self.gaussians):\n",
    "\n",
    "            prior = nweights[k]\n",
    "            #print(\"prior_weight\",prior)\n",
    "            self.likeli = multivariate_normal(nmean[k], nvar[k]).pdf(X)\n",
    "            #print(\"Likelihood/probability\"+str(k),likelihood)\n",
    "\n",
    "            self.probs1[:,k]= prior * self.likeli\n",
    "            #print(\" Size of Initial Probability of all the datapoints in cluster\"+str(k),self.probability.shape)\n",
    "       \n",
    "\n",
    "        self.probs1 = self.probs1 / (np.sum(self.probs1, axis=1)[:, np.newaxis])\n",
    "        \n",
    "        return self.probs1\n",
    "\n",
    "\n",
    "    def Mstep(self, X):\n",
    "        \"\"\"M-step, update parameters\"\"\"\n",
    "\n",
    "        # total probability assigned to each cluster, Soft alocation(N^soft)\n",
    "        #print(\"probability assigned to each cluster\",self.probability.sum(axis = 0))\n",
    "        resp_weights = self.probability.sum(axis = 0)\n",
    "        \n",
    "        # updated_weights\n",
    "        self.weights = resp_weights / X.shape[0]\n",
    "\n",
    "        # updated_means\n",
    "        weighted_sum = np.dot(self.probability.T, X)\n",
    "        self.means = weighted_sum / resp_weights.reshape(-1, 1)\n",
    "        # updated_covariance\n",
    "        for k in range(self.gaussians):\n",
    "            diff = (X - self.means[k]).T\n",
    "            weighted_sum = np.dot(self.probability[:, k] * diff, diff.T)\n",
    "            self.covs[k] = weighted_sum / resp_weights[k]\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "       \n",
    "        post_proba = np.zeros((X.shape[0], self.gaussians))\n",
    "        \n",
    "        for c in range(self.gaussians):\n",
    "            post_proba [:,c] = self.weights[c] * multivariate_normal.pdf(X, self.means[c,:], self.covs[c])\n",
    "            #print(\"Posterior_probability:\", post_proba)\n",
    "        labels  =  post_proba.argmax(1)\n",
    "        #print(\"Labels/Classes:\",labels)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=4\n",
    "model = GaussianMixture(gaussians=K, n_iters = 50, tol = 0.0, seed = 4)\n",
    "#fitted_values = model.fit(X)\n",
    "mean,weight,covar,probability = model.fit(X)\n",
    "print(\"Means\",mean)\n",
    "print(\"Weights\",weight)\n",
    "print(\"Covs\",covar)\n",
    "print(\"Posterior Probability:\\n\",probability)\n",
    "\n",
    "UpdatedPosteriorprobability=model.compute_log_likelihood(X)\n",
    "print(\"\\nLast_Iteration_Posteriorprobability\",UpdatedPosteriorprobability)\n",
    "print(\"\\nSum Of Last_Iteration_Posteriorprobability\",UpdatedPosteriorprobability.sum(1))\n",
    "\n",
    "predicted_values = model.predict(X)\n",
    "print(\"\\nClassified cluster labels\\n\",predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.zeros((K,2))\n",
    "for i in range(model.gaussians):\n",
    "    density = multivariate_normal(cov=model.covs[i], mean=model.means[i]).logpdf(X)\n",
    "    centers[i, :] = X[np.argmax(density)]\n",
    "    \n",
    "plt.figure(figsize = (10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1],c=predicted_values ,s=10, cmap='viridis')\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1],c='Red', s=300, alpha=0.6);\n",
    "print('converged iteration:', len(model.log_likelihood_trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1],c=predicted_values ,s=5, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1],c='Red', s=300, alpha=0.6);\n",
    "\n",
    "w_factor = 0.2 / model.weights.max()\n",
    "for pos, covar, w in zip(model.means, model.covs, model.weights):\n",
    "    draw_ellipse(pos, covar, alpha=w * w_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "gmm = mixture.GaussianMixture(n_components = 4, covariance_type = 'full', \n",
    "                      max_iter = 600, random_state = 3)\n",
    "\n",
    "gmm.fit(X)\n",
    "\n",
    "print('converged or not: ', gmm.converged_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation wrt original GMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import mixture\n",
    "model = mixture.GaussianMixture(n_components=4, covariance_type='full').fit(Data)\n",
    "labels = model.predict(X)\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=5, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
